Project: Web Crawler

Description:
Goal was to implement a web crawler that collects five secret flags by crawling fakebook. Initial task was to successfully login to the website using the http GET and POST methods. After inspecting the form's code and setting the cookie field to the csrf middleware token, we used the POST method to submit the form data to the server, for which we got a response from the server with the session id set for future exchange of information. Using the sessionid as the second name value pair for cookie field(the first being the csrf middleware token) we could succesfully login to fakebook. Once logged in, we crawled all the pages starting from the seed page /fakebook/. Starting from the seed page we obtained all the links from that page and added the home page to crawled(list of crawled pages) list and the list of obtained links/pages to the tocrawl(pages to be crawled) list. We then requested the content for all the obtained pages by filtering out pages that were already in the list of crawled pages.By doing this repeatedly we searched for the FLAG field every time we got the content from a particular page. The crawler terminates after all the 5 secret flags have been obtained and the tocrawl list ends up being empty. The approach we used to crawl the pages is the standard Breadth First Search approach where all the pages can be imagined to be connected through links in the form of a tree. Our program is designed to handle different status codes that the server might throw like 301- Moved permanently, 403- Forbidden, 404- Not Found and 500- Internal Server error for which we spent a lot of time and felt was the most challenging part of this project.
